{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2f0c19f",
   "metadata": {
    "papermill": {
     "duration": 0.004122,
     "end_time": "2022-07-06T15:09:30.795327",
     "exception": false,
     "start_time": "2022-07-06T15:09:30.791205",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Lag Features Are All You Need\n",
    "\n",
    "> **Credits:** Based on [this](https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7963) amazing notebook.\n",
    "\n",
    "OK. Maybe not **all** you need.\n",
    "<br>\n",
    "But they improve `LightGBM`!\n",
    "_____\n",
    "\n",
    "\n",
    "This notebook stated as an ensemble of `LightGBM` + `Catboost` + `XGB` but while running it I discovered an interestin idea that worked really well.\n",
    "\n",
    "### Lag Features\n",
    "\n",
    "On this competition we get information about clients of AMEX over time. \n",
    "Most high scoring notebooks on this competiion focused on aggregating the information per client and create a single row of extracted features: One for each client.\n",
    "\n",
    "**One of such agg function is `last`**.\n",
    "\n",
    "Quick examination revealed that the `last` feature is extreamly powerful at predicting if the client defaults or not (well.. make sense..). \n",
    "So I took this two steps further: \n",
    "\n",
    "- **First feature:** Just like the `last` feature: I added a `first` feature. \n",
    "- **\"Lag\" fearures:** to capture the change over time about each client I calculated two features for every `first`, `last` pair:\n",
    "     - **Last - First:** The change since we first see the client to the last time we see the client.\n",
    "     - **Last / First:** The fractional difference since we first see the client to the last time we see the client.\n",
    "\n",
    "This improved my `LightGBM` model to the point that it overtook the whole `LightGBM` + `Catboost` + `XGB` ensemble.\n",
    "\n",
    "I uploaded a dataset containing the extracted lag features and updated the final model predictions (only `LightGBM` this time) for everyone to play with. \n",
    "\n",
    "<br>\n",
    "\n",
    "_____\n",
    "\n",
    "**Next Experiement (currently running):** More \"lag features\" variations - Also take in consideration other indices of the time-series. will keep you updated.\n",
    "_____\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a524a715",
   "metadata": {
    "papermill": {
     "duration": 0.002645,
     "end_time": "2022-07-06T15:09:30.803906",
     "exception": false,
     "start_time": "2022-07-06T15:09:30.801261",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8043d21e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-06T15:09:30.812870Z",
     "iopub.status.busy": "2022-07-06T15:09:30.812438Z",
     "iopub.status.idle": "2022-07-06T15:09:30.995590Z",
     "shell.execute_reply": "2022-07-06T15:09:30.994151Z"
    },
    "papermill": {
     "duration": 0.191822,
     "end_time": "2022-07-06T15:09:30.998606",
     "exception": false,
     "start_time": "2022-07-06T15:09:30.806784",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "# ====================================================\n",
    "# Read & preprocess data and save it to disk\n",
    "# ====================================================\n",
    "def read_preprocess_data():\n",
    "    train = pd.read_parquet('../input/amex-data-integer-dtypes-parquet-format/train.parquet')\n",
    "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\",\n",
    "    ]\n",
    "    num_features = [col for col in features if col not in cat_features]\n",
    "    \n",
    "    # Train FE\n",
    "    print('Starting train feature extraction')\n",
    "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
    "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "    train_num_agg.reset_index(inplace = True)\n",
    "\n",
    "    # Lag Features\n",
    "    for col in train_num_agg:\n",
    "        if 'last' in col and col.replace('last', 'first') in train_num_agg:\n",
    "            train_num_agg[col + '_lag_sub'] = train_num_agg[col] - train_num_agg[col.replace('last', 'first')]\n",
    "            train_num_agg[col + '_lag_div'] = train_num_agg[col] / train_num_agg[col.replace('last', 'first')]\n",
    "\n",
    "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'first', 'last', 'nunique'])\n",
    "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "    train_cat_agg.reset_index(inplace = True)\n",
    "    \n",
    "    train_labels = pd.read_csv('../input/amex-default-prediction/train_labels.csv')\n",
    "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
    "    print('Train shape: ', train.shape)    \n",
    "    del train_num_agg, train_cat_agg        \n",
    "    gc.collect()\n",
    "    \n",
    "    # Test FE\n",
    "    test = pd.read_parquet('../input/amex-data-integer-dtypes-parquet-format/test.parquet')\n",
    "    print('Starting test feature extraction')\n",
    "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    test_num_agg.reset_index(inplace = True)\n",
    "\n",
    "    # Lag Features\n",
    "    for col in test_num_agg:\n",
    "        if 'last' in col and col.replace('last', 'first') in test_num_agg:\n",
    "            test_num_agg[col + '_lag_sub'] = test_num_agg[col] - test_num_agg[col.replace('last', 'first')]\n",
    "            test_num_agg[col + '_lag_div'] = test_num_agg[col] / test_num_agg[col.replace('last', 'first')]\n",
    "\n",
    "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'first', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    test_cat_agg.reset_index(inplace = True)\n",
    "    \n",
    "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID')\n",
    "    print('Test shape: ', test.shape)\n",
    "    del test_num_agg, test_cat_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    # Save files to disk\n",
    "    train.to_parquet('train_fe_plus_plus.parquet')\n",
    "    test.to_parquet('test_fe_plus_plus.parquet')\n",
    "    \n",
    "# Read & Preprocess Data\n",
    "# read_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1194b79",
   "metadata": {
    "papermill": {
     "duration": 0.002685,
     "end_time": "2022-07-06T15:09:31.004476",
     "exception": false,
     "start_time": "2022-07-06T15:09:31.001791",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7afcdd92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T15:09:31.014704Z",
     "iopub.status.busy": "2022-07-06T15:09:31.014115Z",
     "iopub.status.idle": "2022-07-06T15:09:33.679478Z",
     "shell.execute_reply": "2022-07-06T15:09:33.678164Z"
    },
    "papermill": {
     "duration": 2.673873,
     "end_time": "2022-07-06T15:09:33.682611",
     "exception": false,
     "start_time": "2022-07-06T15:09:31.008738",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import random\n",
    "import warnings\n",
    "import itertools\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "warnings.filterwarnings('ignore')\n",
    "from itertools import combinations\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "from catboost import CatBoostClassifier\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "class CFG:\n",
    "    input_dir = '../input/amex-fe/'\n",
    "    seed = 42\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + 'train_fe_plus_plus.parquet')\n",
    "    test = pd.read_parquet(CFG.input_dir + 'test_fe_plus_plus.parquet')\n",
    "    return train, test\n",
    "\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "def amex_metric_np(preds, target):\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3432634",
   "metadata": {
    "papermill": {
     "duration": 0.003511,
     "end_time": "2022-07-06T15:09:33.690195",
     "exception": false,
     "start_time": "2022-07-06T15:09:33.686684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training LightGBM (DART) Model\n",
    "\n",
    "- Final predictions output uploaded as a public dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98661069",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-07-06T15:09:33.698380Z",
     "iopub.status.busy": "2022-07-06T15:09:33.697943Z",
     "iopub.status.idle": "2022-07-06T15:09:33.719006Z",
     "shell.execute_reply": "2022-07-06T15:09:33.717770Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.028439,
     "end_time": "2022-07-06T15:09:33.721871",
     "exception": false,
     "start_time": "2022-07-06T15:09:33.693432",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n",
    "\n",
    "def train_and_evaluate(train, test):\n",
    "    # Label encode categorical features\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\"\n",
    "    ]\n",
    "    cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "    for cat_col in cat_features:\n",
    "        encoder = LabelEncoder()\n",
    "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "        test[cat_col] = encoder.transform(test[cat_col])\n",
    "    # Round last float features to 2 decimal place\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    num_cols = [col for col in num_cols if 'last' in col]\n",
    "    for col in num_cols:\n",
    "        train[col + '_round2'] = train[col].round(2)\n",
    "        test[col + '_round2'] = test[col].round(2)\n",
    "    # Get feature list\n",
    "    features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': \"binary_logloss\",\n",
    "        'boosting': 'dart',\n",
    "        'seed': CFG.seed,\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.20,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 2,\n",
    "        'min_data_in_leaf': 40\n",
    "        }\n",
    "    # Create a numpy array to store test predictions\n",
    "    test_predictions = np.zeros(len(test))\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold} with {len(features)} features...')\n",
    "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 10500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 500,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        # Save best model\n",
    "        joblib.dump(model, f'lgbm_fold{fold}_seed{CFG.seed}.pkl')\n",
    "        # Predict validation\n",
    "        val_pred = model.predict(x_val)\n",
    "        # Add to out of folds array\n",
    "        oof_predictions[val_ind] = val_pred\n",
    "        # Predict the test set\n",
    "        test_pred = model.predict(test[features])\n",
    "        test_predictions += test_pred / CFG.n_folds\n",
    "        # Compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(f'Our fold {fold} CV score is {score}')\n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "    # Compute out of folds metric\n",
    "    score = amex_metric(train[CFG.target], oof_predictions)\n",
    "    print(f'Our out of folds CV score is {score}')\n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "    oof_df.to_csv(f'oof_lgbm_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "    # Create a dataframe to store test prediction\n",
    "    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "    test_df.to_csv(f'test_lgbm_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "# seed_everything(CFG.seed)\n",
    "# train, test = read_data()\n",
    "# train_and_evaluate(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c531ca",
   "metadata": {
    "papermill": {
     "duration": 0.002816,
     "end_time": "2022-07-06T15:09:33.728277",
     "exception": false,
     "start_time": "2022-07-06T15:09:33.725461",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Prediction\n",
    "\n",
    "- Replace / comment-out this to use your own predictions from the model in the above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12f9f80a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T15:09:33.735990Z",
     "iopub.status.busy": "2022-07-06T15:09:33.735628Z",
     "iopub.status.idle": "2022-07-06T15:09:39.071118Z",
     "shell.execute_reply": "2022-07-06T15:09:39.070202Z"
    },
    "papermill": {
     "duration": 5.342453,
     "end_time": "2022-07-06T15:09:39.073730",
     "exception": false,
     "start_time": "2022-07-06T15:09:33.731277",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df_1 = pd.read_csv('../input/amex-predictions/test_lgbm_baseline_5fold_seed42.csv')\n",
    "df_1.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.738938,
   "end_time": "2022-07-06T15:09:40.000109",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-06T15:09:20.261171",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
